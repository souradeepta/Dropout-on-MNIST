{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JZ4xPUNztps"
   },
   "source": [
    "# <font color=\"TURQUOISE\">Dropout Exercise</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 676,
     "status": "error",
     "timestamp": 1550714868459,
     "user": {
      "displayName": "Souradeepta Biswas",
      "photoUrl": "https://lh5.googleusercontent.com/-8iBJB6yxUuY/AAAAAAAAAAI/AAAAAAAAFIM/TkUD5spEYUA/s64/photo.jpg",
      "userId": "05312634483470100056"
     },
     "user_tz": 300
    },
    "id": "5vQsU6LYztqN",
    "outputId": "58e762d4-7b82-4256-c175-5eb21bcafdcc"
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "mx.random.seed(1)\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfPkWaDnztqi"
   },
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4rc13grztqk",
    "outputId": "e6d50afd-d612-4a52-8cb9-f13848ad80de"
   },
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbc2X5t5ztqs"
   },
   "outputs": [],
   "source": [
    "W1 = nd.random_normal(shape=(784,256), ctx=ctx) *.01\n",
    "b1 = nd.random_normal(shape=256, ctx=ctx) * .01\n",
    "\n",
    "W2 = nd.random_normal(shape=(256,128), ctx=ctx) *.01\n",
    "b2 = nd.random_normal(shape=128, ctx=ctx) * .01\n",
    "\n",
    "W3 = nd.random_normal(shape=(128,10), ctx=ctx) *.01\n",
    "b3 = nd.random_normal(shape=10, ctx=ctx) *.01\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3foEqrIztqz"
   },
   "source": [
    "### Allocate space for gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INWBO6ASztq0"
   },
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeKlQhBbztq7"
   },
   "source": [
    "### ReLU Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaPk82aHztq-"
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgrYvaUZztrF"
   },
   "source": [
    "# <font color=\"DODGERBLUE\">Dropout Function definition</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate a random number < keep probability\n",
    "### 2. Multiply drop matrix with conv output\n",
    "### 3. Scale this output according to keep probability to bump output the same dimension as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npw-XafeztrJ"
   },
   "outputs": [],
   "source": [
    "def dropout(X, drop_probability):\n",
    "    keep_probability = 1 - drop_probability\n",
    "    # generate dropout mask based on keep probability\n",
    "    mask = nd.random_uniform(0, 1.0, X.shape, ctx=X.context) < keep_probability\n",
    "    #############################\n",
    "    #  Avoid division by 0 when scaling\n",
    "    #############################\n",
    "    if keep_probability > 0.0:\n",
    "        scale = (1/keep_probability)\n",
    "    else:\n",
    "        scale = 0.0\n",
    "    # multiply conv layer with dropout and scale\n",
    "    return mask * X * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"LAWNGREEN\">Dropout of 0 keeps all values in</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AYdnStXztrR",
    "outputId": "ce6f5c82-5dad-4b4e-fc03-db666ddc3d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  1.  2.  3.]\n",
       " [ 4.  5.  6.  7.]\n",
       " [ 8.  9. 10. 11.]\n",
       " [12. 13. 14. 15.]\n",
       " [16. 17. 18. 19.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.arange(20).reshape((5,4))\n",
    "dropout(A, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"GOLD\">Dropout of half keeps half values in</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MU4qPa3Bztrb",
    "outputId": "aa523139-a10c-4179-fb36-2752d7899ff8",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.  2.  4.  6.]\n",
       " [ 8.  0. 12. 14.]\n",
       " [16. 18. 20. 22.]\n",
       " [24.  0. 28. 30.]\n",
       " [32. 34.  0. 38.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(A, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"ORANGERED\">Dropout of 1 drops all values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfUgm_gVztrl",
    "outputId": "58afbbb0-9bfe-4a8a-ffb2-15a3c0500c05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]\n",
       " [0. 0. 0. 0.]]\n",
       "<NDArray 5x4 @cpu(0)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(A, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCe_GsLhztru"
   },
   "source": [
    "### Softmax output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6KBuBKXztry"
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9x3FcGFBztr6"
   },
   "source": [
    "### The softmax cross-entropy loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RGfaMBtztr-"
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Do3bQ9MiztsH"
   },
   "source": [
    "### Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGSDnuo8ztsL"
   },
   "outputs": [],
   "source": [
    "def net(X, drop_prob=0.0):\n",
    "    #######################\n",
    "    #  Compute the first hidden layer\n",
    "    #######################\n",
    "    h1_linear = nd.dot(X, W1) + b1\n",
    "    h1 = relu(h1_linear)\n",
    "    h1 = dropout(h1, drop_prob)\n",
    "\n",
    "    #######################\n",
    "    #  Compute the second hidden layer\n",
    "    #######################\n",
    "    h2_linear = nd.dot(h1, W2) + b2\n",
    "    h2 = relu(h2_linear)\n",
    "    h2 = dropout(h2, drop_prob)\n",
    "\n",
    "    #######################\n",
    "    #  Compute the output layer.\n",
    "    #  We will omit the softmax function here\n",
    "    #  because it will be applied\n",
    "    #  in the softmax_cross_entropy loss\n",
    "    #######################\n",
    "    yhat_linear = nd.dot(h2, W3) + b3\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnlwUp6Dztsa"
   },
   "source": [
    "### Optimizer: Stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owfgeO1aztsi"
   },
   "outputs": [],
   "source": [
    "def SGD(params, learning_rate):\n",
    "    for param in params:\n",
    "        param[:] = param - learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUNv4I2Iztuo"
   },
   "source": [
    "### Evaluation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxyDz3GYztuv"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1,784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UoVI4WNvztu7"
   },
   "source": [
    "### Execute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CeJgmSiCztu_"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "moving_loss = 0.\n",
    "learning_rate = .001\n",
    "\n",
    "def train(net, drop_prob, epochs):\n",
    "    for e in range(epochs):\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            data = data.as_in_context(ctx).reshape((-1,784))\n",
    "            label = label.as_in_context(ctx)\n",
    "            label_one_hot = nd.one_hot(label, 10)\n",
    "            with autograd.record():\n",
    "                ########################\n",
    "                #   Drop out used here #\n",
    "                #######################\n",
    "                output = net(data, drop_prob)\n",
    "                loss = softmax_cross_entropy(output, label_one_hot)\n",
    "            loss.backward()\n",
    "            SGD(params, learning_rate)\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if i == 0:\n",
    "                moving_loss = nd.mean(loss).asscalar()\n",
    "            else:\n",
    "                moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "\n",
    "        test_accuracy = evaluate_accuracy(test_data, net)\n",
    "        train_accuracy = evaluate_accuracy(train_data, net)\n",
    "        print(\"Epoch %s. Loss: %s,    Train_acc %s,    Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"ORANGERED\">High dropout used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.0959734010353634,    Train_acc 0.7847,    Test_acc 0.7947\n",
      "Epoch 1. Loss: 0.6911433717565789,    Train_acc 0.9026,    Test_acc 0.9054\n",
      "Epoch 2. Loss: 0.5690453843627099,    Train_acc 0.9235333,    Test_acc 0.9235\n",
      "Epoch 3. Loss: 0.5147457278797349,    Train_acc 0.93505,    Test_acc 0.936\n",
      "Epoch 4. Loss: 0.4951578293524433,    Train_acc 0.93995,    Test_acc 0.9407\n",
      "Epoch 5. Loss: 0.4433863431392614,    Train_acc 0.9443667,    Test_acc 0.9424\n",
      "Epoch 6. Loss: 0.43610409423565655,    Train_acc 0.9493333,    Test_acc 0.949\n",
      "Epoch 7. Loss: 0.42300142204891145,    Train_acc 0.95195,    Test_acc 0.9505\n"
     ]
    }
   ],
   "source": [
    "train(net,0.8, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"GOLD\">Dropout of half used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWTtNuyoztvG",
    "outputId": "6e367978-3df2-48f8-838d-5f920ffc4e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.05844953087192618,    Train_acc 0.99675,    Test_acc 0.9805\n",
      "Epoch 1. Loss: 0.05593816106680461,    Train_acc 0.99703336,    Test_acc 0.9817\n",
      "Epoch 2. Loss: 0.05417170336255669,    Train_acc 0.99675,    Test_acc 0.9805\n",
      "Epoch 3. Loss: 0.053125804223949635,    Train_acc 0.99721664,    Test_acc 0.9818\n",
      "Epoch 4. Loss: 0.05272426012394333,    Train_acc 0.9971333,    Test_acc 0.981\n",
      "Epoch 5. Loss: 0.0517162056513185,    Train_acc 0.99698335,    Test_acc 0.9808\n",
      "Epoch 6. Loss: 0.05335260935578101,    Train_acc 0.99766666,    Test_acc 0.9818\n",
      "Epoch 7. Loss: 0.05498305921293919,    Train_acc 0.9978333,    Test_acc 0.9809\n",
      "Epoch 8. Loss: 0.05104374116619116,    Train_acc 0.99715,    Test_acc 0.9815\n",
      "Epoch 9. Loss: 0.05246860679119453,    Train_acc 0.99733335,    Test_acc 0.9811\n"
     ]
    }
   ],
   "source": [
    "train(net,0.5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5j9YqStztvW"
   },
   "source": [
    "# <font color=\"LAWNGREEN\">Dropout of 0 used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Pau40MCztve",
    "outputId": "424ba67c-68ee-4dde-e81e-749c52126823",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.007049042951271868,    Train_acc 0.9992833,    Test_acc 0.9831\n",
      "Epoch 1. Loss: 0.005208811431898914,    Train_acc 0.99951667,    Test_acc 0.9829\n",
      "Epoch 2. Loss: 0.0030589178185008122,    Train_acc 0.99971664,    Test_acc 0.983\n",
      "Epoch 3. Loss: 0.003068211641881961,    Train_acc 0.99985,    Test_acc 0.9829\n",
      "Epoch 4. Loss: 0.002408713933403444,    Train_acc 0.99981666,    Test_acc 0.9832\n",
      "Epoch 5. Loss: 0.0022319511442104173,    Train_acc 0.99995,    Test_acc 0.9829\n",
      "Epoch 6. Loss: 0.001691517848392048,    Train_acc 0.99995,    Test_acc 0.9834\n",
      "Epoch 7. Loss: 0.0015038096210987341,    Train_acc 0.99995,    Test_acc 0.9831\n",
      "Epoch 8. Loss: 0.0011653935781116094,    Train_acc 0.9999667,    Test_acc 0.9832\n",
      "Epoch 9. Loss: 0.0009982405403038616,    Train_acc 0.9999833,    Test_acc 0.9833\n"
     ]
    }
   ],
   "source": [
    "train(net,0.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"ORANGERED\">High dropout of 0.8 used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTarNZZwztvp",
    "outputId": "466685cc-54a1-4632-98f0-b42bfb3aed02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.5086358352008217,    Train_acc 0.97995,    Test_acc 0.9691\n",
      "Epoch 1. Loss: 0.4614239824096406,    Train_acc 0.97723335,    Test_acc 0.969\n",
      "Epoch 2. Loss: 0.48122709564350097,    Train_acc 0.97618335,    Test_acc 0.9699\n",
      "Epoch 3. Loss: 0.4152667391015395,    Train_acc 0.9749,    Test_acc 0.9674\n",
      "Epoch 4. Loss: 0.4276696791554884,    Train_acc 0.976,    Test_acc 0.9669\n",
      "Epoch 5. Loss: 0.4127018304774338,    Train_acc 0.9751,    Test_acc 0.9671\n",
      "Epoch 6. Loss: 0.39011731180992815,    Train_acc 0.97478336,    Test_acc 0.9678\n",
      "Epoch 7. Loss: 0.38137675653435726,    Train_acc 0.97496665,    Test_acc 0.9693\n",
      "Epoch 8. Loss: 0.3936920239281544,    Train_acc 0.97433335,    Test_acc 0.9671\n",
      "Epoch 9. Loss: 0.38101606456571346,    Train_acc 0.97505,    Test_acc 0.9672\n",
      "Epoch 10. Loss: 0.3633826508275447,    Train_acc 0.97426665,    Test_acc 0.9665\n",
      "Epoch 11. Loss: 0.37900348845824233,    Train_acc 0.97545,    Test_acc 0.9691\n",
      "Epoch 12. Loss: 0.3398188990244026,    Train_acc 0.97578335,    Test_acc 0.9685\n",
      "Epoch 13. Loss: 0.33507752024063786,    Train_acc 0.9759833,    Test_acc 0.9693\n",
      "Epoch 14. Loss: 0.333005060521837,    Train_acc 0.97653335,    Test_acc 0.9695\n",
      "Epoch 15. Loss: 0.3366318835973684,    Train_acc 0.9755167,    Test_acc 0.9673\n",
      "Epoch 16. Loss: 0.33503825310053326,    Train_acc 0.9768,    Test_acc 0.9681\n",
      "Epoch 17. Loss: 0.32850786878338045,    Train_acc 0.9781167,    Test_acc 0.97\n",
      "Epoch 18. Loss: 0.3350772299534577,    Train_acc 0.97713333,    Test_acc 0.9682\n",
      "Epoch 19. Loss: 0.3322047286323204,    Train_acc 0.97651666,    Test_acc 0.9687\n",
      "Epoch 20. Loss: 0.317646746039191,    Train_acc 0.97705,    Test_acc 0.9691\n",
      "Epoch 21. Loss: 0.3174969287003735,    Train_acc 0.97671664,    Test_acc 0.9677\n",
      "Epoch 22. Loss: 0.3280287545908858,    Train_acc 0.97765,    Test_acc 0.9698\n",
      "Epoch 23. Loss: 0.31209073915485563,    Train_acc 0.97625,    Test_acc 0.9678\n",
      "Epoch 24. Loss: 0.3160937744253525,    Train_acc 0.9772,    Test_acc 0.9681\n",
      "Epoch 25. Loss: 0.3114771277222284,    Train_acc 0.97645,    Test_acc 0.968\n",
      "Epoch 26. Loss: 0.313041070255397,    Train_acc 0.9767333,    Test_acc 0.9691\n",
      "Epoch 27. Loss: 0.3138841091678457,    Train_acc 0.9767333,    Test_acc 0.9693\n",
      "Epoch 28. Loss: 0.30215754002570516,    Train_acc 0.97695,    Test_acc 0.9691\n",
      "Epoch 29. Loss: 0.3074354073222522,    Train_acc 0.97755,    Test_acc 0.9687\n",
      "Epoch 30. Loss: 0.29928317232245094,    Train_acc 0.97833335,    Test_acc 0.9699\n",
      "Epoch 31. Loss: 0.2992494467150859,    Train_acc 0.9780667,    Test_acc 0.9699\n",
      "Epoch 32. Loss: 0.29741703501677064,    Train_acc 0.97866666,    Test_acc 0.9697\n",
      "Epoch 33. Loss: 0.29890574689373667,    Train_acc 0.97758335,    Test_acc 0.9696\n",
      "Epoch 34. Loss: 0.30230885201102053,    Train_acc 0.9784167,    Test_acc 0.9692\n",
      "Epoch 35. Loss: 0.2915015385563372,    Train_acc 0.9787167,    Test_acc 0.9696\n",
      "Epoch 36. Loss: 0.2980838983974273,    Train_acc 0.97858334,    Test_acc 0.969\n",
      "Epoch 37. Loss: 0.3107469588978086,    Train_acc 0.9791333,    Test_acc 0.9702\n",
      "Epoch 38. Loss: 0.3012104730804702,    Train_acc 0.97915,    Test_acc 0.97\n",
      "Epoch 39. Loss: 0.28655266641337757,    Train_acc 0.97976667,    Test_acc 0.97\n",
      "Epoch 40. Loss: 0.29460638693282976,    Train_acc 0.97915,    Test_acc 0.9698\n",
      "Epoch 41. Loss: 0.27376668859368963,    Train_acc 0.97933334,    Test_acc 0.9688\n",
      "Epoch 42. Loss: 0.28540297958560185,    Train_acc 0.9788333,    Test_acc 0.9702\n",
      "Epoch 43. Loss: 0.28556974233631754,    Train_acc 0.9798667,    Test_acc 0.9702\n",
      "Epoch 44. Loss: 0.2967230143705026,    Train_acc 0.9795333,    Test_acc 0.9701\n",
      "Epoch 45. Loss: 0.283115111561959,    Train_acc 0.97903335,    Test_acc 0.969\n",
      "Epoch 46. Loss: 0.2936129022181202,    Train_acc 0.98015,    Test_acc 0.9702\n",
      "Epoch 47. Loss: 0.281073411346375,    Train_acc 0.97945,    Test_acc 0.9687\n",
      "Epoch 48. Loss: 0.2772548351619585,    Train_acc 0.9798667,    Test_acc 0.9702\n",
      "Epoch 49. Loss: 0.27829522336650375,    Train_acc 0.97936666,    Test_acc 0.9703\n",
      "Epoch 50. Loss: 0.278773051253338,    Train_acc 0.98001665,    Test_acc 0.9695\n",
      "Epoch 51. Loss: 0.28720657897213253,    Train_acc 0.97971666,    Test_acc 0.9687\n",
      "Epoch 52. Loss: 0.2743063885315234,    Train_acc 0.98083335,    Test_acc 0.9697\n",
      "Epoch 53. Loss: 0.28356492087749713,    Train_acc 0.9812667,    Test_acc 0.9707\n",
      "Epoch 54. Loss: 0.2741545831628012,    Train_acc 0.9802333,    Test_acc 0.9702\n",
      "Epoch 55. Loss: 0.28537098238121156,    Train_acc 0.9798167,    Test_acc 0.9701\n",
      "Epoch 56. Loss: 0.281656371074835,    Train_acc 0.9806333,    Test_acc 0.9695\n",
      "Epoch 57. Loss: 0.27838488588086296,    Train_acc 0.98053336,    Test_acc 0.97\n",
      "Epoch 58. Loss: 0.2672139472923955,    Train_acc 0.9799833,    Test_acc 0.9706\n",
      "Epoch 59. Loss: 0.26964885217426504,    Train_acc 0.9809333,    Test_acc 0.9702\n",
      "Epoch 60. Loss: 0.27122813045379396,    Train_acc 0.98066664,    Test_acc 0.9692\n",
      "Epoch 61. Loss: 0.26479531999210987,    Train_acc 0.9803,    Test_acc 0.9684\n",
      "Epoch 62. Loss: 0.26758172015752235,    Train_acc 0.9805,    Test_acc 0.968\n",
      "Epoch 63. Loss: 0.2812660158662902,    Train_acc 0.98123336,    Test_acc 0.9703\n",
      "Epoch 64. Loss: 0.25069490838479663,    Train_acc 0.98185,    Test_acc 0.9707\n",
      "Epoch 65. Loss: 0.2662006841841241,    Train_acc 0.9812833,    Test_acc 0.9706\n",
      "Epoch 66. Loss: 0.2780299240497769,    Train_acc 0.9809333,    Test_acc 0.9708\n",
      "Epoch 67. Loss: 0.27533969320750357,    Train_acc 0.98193336,    Test_acc 0.9715\n",
      "Epoch 68. Loss: 0.26806507701990345,    Train_acc 0.9821333,    Test_acc 0.9692\n",
      "Epoch 69. Loss: 0.2741604188364432,    Train_acc 0.98125,    Test_acc 0.9697\n",
      "Epoch 70. Loss: 0.2708939819021885,    Train_acc 0.9814,    Test_acc 0.9696\n",
      "Epoch 71. Loss: 0.26559352750400783,    Train_acc 0.98123336,    Test_acc 0.9696\n",
      "Epoch 72. Loss: 0.2636058278197516,    Train_acc 0.98151666,    Test_acc 0.9686\n",
      "Epoch 73. Loss: 0.2514357798053893,    Train_acc 0.9822,    Test_acc 0.9701\n",
      "Epoch 74. Loss: 0.25130298555922165,    Train_acc 0.9819667,    Test_acc 0.9699\n",
      "Epoch 75. Loss: 0.2581274639678034,    Train_acc 0.98088336,    Test_acc 0.9676\n",
      "Epoch 76. Loss: 0.2619701549353818,    Train_acc 0.9821333,    Test_acc 0.9693\n",
      "Epoch 77. Loss: 0.2676022960610837,    Train_acc 0.9826,    Test_acc 0.9707\n",
      "Epoch 78. Loss: 0.26547027401194223,    Train_acc 0.9816833,    Test_acc 0.9686\n",
      "Epoch 79. Loss: 0.2753368980625781,    Train_acc 0.98143333,    Test_acc 0.9695\n",
      "Epoch 80. Loss: 0.25761983099380464,    Train_acc 0.98258334,    Test_acc 0.9689\n",
      "Epoch 81. Loss: 0.25134166648966433,    Train_acc 0.9824833,    Test_acc 0.9706\n",
      "Epoch 82. Loss: 0.2602012406389477,    Train_acc 0.9820833,    Test_acc 0.9693\n",
      "Epoch 83. Loss: 0.25981514284954965,    Train_acc 0.9820667,    Test_acc 0.9685\n",
      "Epoch 84. Loss: 0.2609213498303734,    Train_acc 0.98258334,    Test_acc 0.9699\n",
      "Epoch 85. Loss: 0.2600056276381489,    Train_acc 0.98288333,    Test_acc 0.9703\n",
      "Epoch 86. Loss: 0.2733168340593612,    Train_acc 0.98228335,    Test_acc 0.9686\n",
      "Epoch 87. Loss: 0.25688925278001246,    Train_acc 0.98355,    Test_acc 0.9706\n",
      "Epoch 88. Loss: 0.2650174919599948,    Train_acc 0.9834,    Test_acc 0.9695\n",
      "Epoch 89. Loss: 0.25252624725656375,    Train_acc 0.9824167,    Test_acc 0.9687\n",
      "Epoch 90. Loss: 0.271131846663407,    Train_acc 0.9820667,    Test_acc 0.9681\n",
      "Epoch 91. Loss: 0.2486264804819444,    Train_acc 0.9828,    Test_acc 0.9699\n",
      "Epoch 92. Loss: 0.25420662664206795,    Train_acc 0.98235,    Test_acc 0.9689\n",
      "Epoch 93. Loss: 0.2522083270544202,    Train_acc 0.98335,    Test_acc 0.9694\n",
      "Epoch 94. Loss: 0.25174353018312945,    Train_acc 0.98263335,    Test_acc 0.9692\n",
      "Epoch 95. Loss: 0.2635352787033441,    Train_acc 0.9816667,    Test_acc 0.969\n",
      "Epoch 96. Loss: 0.2593573423785865,    Train_acc 0.98226666,    Test_acc 0.9695\n",
      "Epoch 97. Loss: 0.25889369077309854,    Train_acc 0.98256665,    Test_acc 0.9696\n",
      "Epoch 98. Loss: 0.25332984857351315,    Train_acc 0.982,    Test_acc 0.9694\n",
      "Epoch 99. Loss: 0.27178201367879834,    Train_acc 0.9820667,    Test_acc 0.9699\n"
     ]
    }
   ],
   "source": [
    "train(net,0.8, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"GOLD\">Dropout of 0.5 used</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAu7xLIdztvz",
    "outputId": "e6c63f54-6874-4452-cfff-2db8bcd78ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.0725543986700366,    Train_acc 0.98753333,    Test_acc 0.9737\n",
      "Epoch 1. Loss: 0.06075809338807031,    Train_acc 0.98931664,    Test_acc 0.9745\n",
      "Epoch 2. Loss: 0.05570570510003326,    Train_acc 0.99018335,    Test_acc 0.9752\n",
      "Epoch 3. Loss: 0.06358640744756218,    Train_acc 0.99116665,    Test_acc 0.9754\n",
      "Epoch 4. Loss: 0.05429037356104726,    Train_acc 0.99165,    Test_acc 0.9756\n",
      "Epoch 5. Loss: 0.04900720837717807,    Train_acc 0.9925,    Test_acc 0.9757\n",
      "Epoch 6. Loss: 0.04591402025927379,    Train_acc 0.99273336,    Test_acc 0.9756\n",
      "Epoch 7. Loss: 0.047352310537359436,    Train_acc 0.9936,    Test_acc 0.9771\n",
      "Epoch 8. Loss: 0.04973145411168545,    Train_acc 0.99336666,    Test_acc 0.9766\n",
      "Epoch 9. Loss: 0.04583497905452421,    Train_acc 0.994,    Test_acc 0.9765\n",
      "Epoch 10. Loss: 0.051114642762894534,    Train_acc 0.9946,    Test_acc 0.9769\n",
      "Epoch 11. Loss: 0.04653329936474795,    Train_acc 0.99476665,    Test_acc 0.9765\n",
      "Epoch 12. Loss: 0.04400529388862444,    Train_acc 0.99453336,    Test_acc 0.9779\n",
      "Epoch 13. Loss: 0.04108880855148901,    Train_acc 0.9949833,    Test_acc 0.977\n",
      "Epoch 14. Loss: 0.041826272433017106,    Train_acc 0.99541664,    Test_acc 0.9766\n",
      "Epoch 15. Loss: 0.037029274862455754,    Train_acc 0.9956333,    Test_acc 0.9787\n",
      "Epoch 16. Loss: 0.04191497421513477,    Train_acc 0.9958,    Test_acc 0.9777\n",
      "Epoch 17. Loss: 0.03397176431695278,    Train_acc 0.9959,    Test_acc 0.9777\n",
      "Epoch 18. Loss: 0.039098837491897,    Train_acc 0.99621665,    Test_acc 0.9782\n",
      "Epoch 19. Loss: 0.03865732816895456,    Train_acc 0.99628335,    Test_acc 0.9777\n",
      "Epoch 20. Loss: 0.03968631547887534,    Train_acc 0.9967667,    Test_acc 0.9773\n",
      "Epoch 21. Loss: 0.03815964150456196,    Train_acc 0.9967667,    Test_acc 0.978\n",
      "Epoch 22. Loss: 0.03772183622478836,    Train_acc 0.99691665,    Test_acc 0.9775\n",
      "Epoch 23. Loss: 0.0395696514867928,    Train_acc 0.9971167,    Test_acc 0.9771\n",
      "Epoch 24. Loss: 0.03283422249759385,    Train_acc 0.9968167,    Test_acc 0.9782\n",
      "Epoch 25. Loss: 0.03398201003940668,    Train_acc 0.9971167,    Test_acc 0.9775\n",
      "Epoch 26. Loss: 0.03038789568203614,    Train_acc 0.9974333,    Test_acc 0.9772\n",
      "Epoch 27. Loss: 0.03426757025018213,    Train_acc 0.99726665,    Test_acc 0.9782\n",
      "Epoch 28. Loss: 0.033954798539023015,    Train_acc 0.9976,    Test_acc 0.9779\n",
      "Epoch 29. Loss: 0.029048951925907243,    Train_acc 0.9976,    Test_acc 0.9776\n",
      "Epoch 30. Loss: 0.03692874760902009,    Train_acc 0.99785,    Test_acc 0.9796\n",
      "Epoch 31. Loss: 0.03197220552404655,    Train_acc 0.9974833,    Test_acc 0.9794\n",
      "Epoch 32. Loss: 0.032408154402113816,    Train_acc 0.99765,    Test_acc 0.9782\n",
      "Epoch 33. Loss: 0.031359644277174005,    Train_acc 0.99785,    Test_acc 0.9781\n",
      "Epoch 34. Loss: 0.032622952664542126,    Train_acc 0.99795,    Test_acc 0.9778\n",
      "Epoch 35. Loss: 0.031956608735763475,    Train_acc 0.99806666,    Test_acc 0.9773\n",
      "Epoch 36. Loss: 0.03295344711516866,    Train_acc 0.9978,    Test_acc 0.9775\n",
      "Epoch 37. Loss: 0.028570640005122397,    Train_acc 0.99813336,    Test_acc 0.9787\n",
      "Epoch 38. Loss: 0.031059750620813602,    Train_acc 0.99833333,    Test_acc 0.9776\n",
      "Epoch 39. Loss: 0.02983723697097622,    Train_acc 0.9982,    Test_acc 0.9788\n",
      "Epoch 40. Loss: 0.03187442638377453,    Train_acc 0.9982333,    Test_acc 0.9793\n",
      "Epoch 41. Loss: 0.03275480612799142,    Train_acc 0.9984,    Test_acc 0.9784\n",
      "Epoch 42. Loss: 0.028354916316999833,    Train_acc 0.9984,    Test_acc 0.9794\n",
      "Epoch 43. Loss: 0.02800895691463429,    Train_acc 0.9985,    Test_acc 0.9781\n",
      "Epoch 44. Loss: 0.029726712262450834,    Train_acc 0.99835,    Test_acc 0.979\n",
      "Epoch 45. Loss: 0.02550885931694951,    Train_acc 0.9985167,    Test_acc 0.9777\n",
      "Epoch 46. Loss: 0.02756272451206224,    Train_acc 0.99868333,    Test_acc 0.9775\n",
      "Epoch 47. Loss: 0.0322782046836519,    Train_acc 0.99845,    Test_acc 0.9777\n",
      "Epoch 48. Loss: 0.028200900242595504,    Train_acc 0.9985,    Test_acc 0.9778\n",
      "Epoch 49. Loss: 0.028696686820368095,    Train_acc 0.99878335,    Test_acc 0.9781\n",
      "Epoch 50. Loss: 0.030492364028359385,    Train_acc 0.99878335,    Test_acc 0.9787\n",
      "Epoch 51. Loss: 0.02805572191493387,    Train_acc 0.99885,    Test_acc 0.9783\n",
      "Epoch 52. Loss: 0.026525942920190577,    Train_acc 0.99883336,    Test_acc 0.9776\n",
      "Epoch 53. Loss: 0.02583942702406532,    Train_acc 0.99878335,    Test_acc 0.9776\n",
      "Epoch 54. Loss: 0.023887217616452776,    Train_acc 0.99881667,    Test_acc 0.978\n",
      "Epoch 55. Loss: 0.03297833449579134,    Train_acc 0.9988,    Test_acc 0.9781\n",
      "Epoch 56. Loss: 0.028144125644112415,    Train_acc 0.9988833,    Test_acc 0.9786\n",
      "Epoch 57. Loss: 0.026095156347031905,    Train_acc 0.999,    Test_acc 0.9789\n",
      "Epoch 58. Loss: 0.027411906652733947,    Train_acc 0.9989167,    Test_acc 0.9787\n",
      "Epoch 59. Loss: 0.024119956556655872,    Train_acc 0.99901664,    Test_acc 0.9784\n",
      "Epoch 60. Loss: 0.027806695175612565,    Train_acc 0.99895,    Test_acc 0.9781\n",
      "Epoch 61. Loss: 0.02454365123188617,    Train_acc 0.9992,    Test_acc 0.9781\n",
      "Epoch 62. Loss: 0.026895504566698182,    Train_acc 0.9991,    Test_acc 0.9779\n",
      "Epoch 63. Loss: 0.023455013214262268,    Train_acc 0.99915,    Test_acc 0.9783\n",
      "Epoch 64. Loss: 0.023593949487269145,    Train_acc 0.9992667,    Test_acc 0.9786\n",
      "Epoch 65. Loss: 0.02223258604704917,    Train_acc 0.99941665,    Test_acc 0.9785\n",
      "Epoch 66. Loss: 0.02628487496763358,    Train_acc 0.99905,    Test_acc 0.9787\n",
      "Epoch 67. Loss: 0.026337960483159423,    Train_acc 0.9992833,    Test_acc 0.9769\n",
      "Epoch 68. Loss: 0.025593925296126818,    Train_acc 0.99948335,    Test_acc 0.9783\n",
      "Epoch 69. Loss: 0.020762082739500205,    Train_acc 0.9992833,    Test_acc 0.9776\n",
      "Epoch 70. Loss: 0.031760752413121936,    Train_acc 0.99916667,    Test_acc 0.9786\n",
      "Epoch 71. Loss: 0.028050124905854523,    Train_acc 0.99911666,    Test_acc 0.9781\n",
      "Epoch 72. Loss: 0.02376378156657797,    Train_acc 0.99916667,    Test_acc 0.9785\n",
      "Epoch 73. Loss: 0.02400829620201067,    Train_acc 0.99925,    Test_acc 0.9782\n",
      "Epoch 74. Loss: 0.020133145757658774,    Train_acc 0.9994,    Test_acc 0.9775\n",
      "Epoch 75. Loss: 0.02687154223947605,    Train_acc 0.9995,    Test_acc 0.9777\n",
      "Epoch 76. Loss: 0.023272423078191894,    Train_acc 0.9992833,    Test_acc 0.9782\n",
      "Epoch 77. Loss: 0.02466414803640647,    Train_acc 0.9995,    Test_acc 0.9779\n",
      "Epoch 78. Loss: 0.02068172069739198,    Train_acc 0.99948335,    Test_acc 0.9783\n",
      "Epoch 79. Loss: 0.024954414854443753,    Train_acc 0.99948335,    Test_acc 0.9781\n",
      "Epoch 80. Loss: 0.023810431394489875,    Train_acc 0.99938333,    Test_acc 0.9782\n",
      "Epoch 81. Loss: 0.023217628245497845,    Train_acc 0.99938333,    Test_acc 0.9788\n",
      "Epoch 82. Loss: 0.022187677020788863,    Train_acc 0.99951667,    Test_acc 0.9783\n",
      "Epoch 83. Loss: 0.019865806517379812,    Train_acc 0.9995833,    Test_acc 0.9791\n",
      "Epoch 84. Loss: 0.017203477261948426,    Train_acc 0.99945,    Test_acc 0.9785\n",
      "Epoch 85. Loss: 0.022346173017349792,    Train_acc 0.99955,    Test_acc 0.9786\n",
      "Epoch 86. Loss: 0.02503830120319748,    Train_acc 0.99948335,    Test_acc 0.9791\n",
      "Epoch 87. Loss: 0.02086185181632038,    Train_acc 0.99945,    Test_acc 0.9778\n",
      "Epoch 88. Loss: 0.025672764735754874,    Train_acc 0.9995667,    Test_acc 0.979\n",
      "Epoch 89. Loss: 0.027582608771123333,    Train_acc 0.99946666,    Test_acc 0.9787\n",
      "Epoch 90. Loss: 0.020531030009729195,    Train_acc 0.9992833,    Test_acc 0.9776\n",
      "Epoch 91. Loss: 0.02376921092704668,    Train_acc 0.9994,    Test_acc 0.9782\n",
      "Epoch 92. Loss: 0.020928050014107025,    Train_acc 0.99948335,    Test_acc 0.9797\n",
      "Epoch 93. Loss: 0.02055054266817824,    Train_acc 0.9995667,    Test_acc 0.9786\n",
      "Epoch 94. Loss: 0.02198254473303878,    Train_acc 0.9996,    Test_acc 0.9786\n",
      "Epoch 95. Loss: 0.023812319498865767,    Train_acc 0.9995,    Test_acc 0.9792\n",
      "Epoch 96. Loss: 0.022404334808274605,    Train_acc 0.9996,    Test_acc 0.9795\n",
      "Epoch 97. Loss: 0.019103405293022062,    Train_acc 0.9996667,    Test_acc 0.9785\n",
      "Epoch 98. Loss: 0.026625714698786624,    Train_acc 0.9995833,    Test_acc 0.9773\n",
      "Epoch 99. Loss: 0.020329581934420252,    Train_acc 0.9995833,    Test_acc 0.9788\n"
     ]
    }
   ],
   "source": [
    "train(net,0.5, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"LAWNGREEN\">NO Dropout used</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model overfits the data since Training accuracy is 100% and testing accuracy is less than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A86VI3KCztv_",
    "outputId": "f7e05b6d-124c-4a6b-c6ea-59e79cf3a5d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.0016375640792185607,    Train_acc 0.99986666,    Test_acc 0.9786\n",
      "Epoch 1. Loss: 0.0014725788864934607,    Train_acc 0.99988335,    Test_acc 0.9789\n",
      "Epoch 2. Loss: 0.0010551783543087992,    Train_acc 0.9999,    Test_acc 0.9788\n",
      "Epoch 3. Loss: 0.0008233518454906051,    Train_acc 0.9999167,    Test_acc 0.9788\n",
      "Epoch 4. Loss: 0.0008872226114330016,    Train_acc 0.9999167,    Test_acc 0.9789\n",
      "Epoch 5. Loss: 0.0007882358503075904,    Train_acc 0.9999167,    Test_acc 0.9788\n",
      "Epoch 6. Loss: 0.0008159225270600517,    Train_acc 0.99993336,    Test_acc 0.979\n",
      "Epoch 7. Loss: 0.0010220072928986237,    Train_acc 0.99995,    Test_acc 0.9791\n",
      "Epoch 8. Loss: 0.0009894596513810158,    Train_acc 0.99995,    Test_acc 0.979\n",
      "Epoch 9. Loss: 0.0005782774989442238,    Train_acc 0.9999667,    Test_acc 0.9792\n",
      "Epoch 10. Loss: 0.0007660663426596448,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 11. Loss: 0.000737330754276713,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 12. Loss: 0.0005676724463170908,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 13. Loss: 0.0004063234941014677,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 14. Loss: 0.0004651802662414292,    Train_acc 0.9999667,    Test_acc 0.9792\n",
      "Epoch 15. Loss: 0.00042561176328762923,    Train_acc 0.9999667,    Test_acc 0.9792\n",
      "Epoch 16. Loss: 0.0008301668734286661,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 17. Loss: 0.0005815568225229141,    Train_acc 0.9999667,    Test_acc 0.9793\n",
      "Epoch 18. Loss: 0.0003435016819612127,    Train_acc 0.9999667,    Test_acc 0.9792\n",
      "Epoch 19. Loss: 0.0003260875609394926,    Train_acc 0.9999667,    Test_acc 0.9792\n",
      "Epoch 20. Loss: 0.0004877919693446338,    Train_acc 0.9999667,    Test_acc 0.9791\n",
      "Epoch 21. Loss: 0.0003629482632324645,    Train_acc 0.9999833,    Test_acc 0.9792\n",
      "Epoch 22. Loss: 0.0005392973347566031,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 23. Loss: 0.00034451945783227833,    Train_acc 0.9999833,    Test_acc 0.9792\n",
      "Epoch 24. Loss: 0.00030359117355878516,    Train_acc 0.9999833,    Test_acc 0.9792\n",
      "Epoch 25. Loss: 0.00027563495590490647,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 26. Loss: 0.00026724448266709637,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 27. Loss: 0.0002480013872858743,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 28. Loss: 0.0003402046744512811,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 29. Loss: 0.00035613380440281005,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 30. Loss: 0.0003392718404081884,    Train_acc 0.9999833,    Test_acc 0.9792\n",
      "Epoch 31. Loss: 0.0003016912451206664,    Train_acc 0.9999833,    Test_acc 0.979\n",
      "Epoch 32. Loss: 0.0002568364103636583,    Train_acc 0.9999833,    Test_acc 0.979\n",
      "Epoch 33. Loss: 0.0003574828684709541,    Train_acc 0.9999833,    Test_acc 0.979\n",
      "Epoch 34. Loss: 0.0002550259628532459,    Train_acc 0.9999833,    Test_acc 0.9791\n",
      "Epoch 35. Loss: 0.00024819002655176553,    Train_acc 0.9999833,    Test_acc 0.9789\n",
      "Epoch 36. Loss: 0.0002834516370373439,    Train_acc 0.9999833,    Test_acc 0.9789\n",
      "Epoch 37. Loss: 0.0002304762304281791,    Train_acc 0.9999833,    Test_acc 0.979\n",
      "Epoch 38. Loss: 0.00025834843654074896,    Train_acc 0.9999833,    Test_acc 0.979\n",
      "Epoch 39. Loss: 0.00020172620382534544,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 40. Loss: 0.0002126212294819455,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 41. Loss: 0.0003071122491140326,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 42. Loss: 0.00022243560571333867,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 43. Loss: 0.00032352179521126875,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 44. Loss: 0.00020140905116025357,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 45. Loss: 0.00020995931792324916,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 46. Loss: 0.00016033141324453922,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 47. Loss: 0.00016705943817874147,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 48. Loss: 0.00015199264124578823,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 49. Loss: 0.00016274336380238055,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 50. Loss: 0.0001969205367928939,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 51. Loss: 0.00019003207167238146,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 52. Loss: 0.0002194634916742662,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 53. Loss: 0.0001597616991674249,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 54. Loss: 0.00019014016274409502,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 55. Loss: 0.0001419092808433471,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 56. Loss: 0.00018347398830580058,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 57. Loss: 0.00015525572829894042,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 58. Loss: 0.00015888882860843568,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 59. Loss: 0.00023898677993733328,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 60. Loss: 0.0001469590580221771,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 61. Loss: 0.00016035208525667463,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 62. Loss: 0.00019813809207133945,    Train_acc 1.0,    Test_acc 0.9787\n",
      "Epoch 63. Loss: 0.00015467647219537074,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 64. Loss: 0.00015445101416291488,    Train_acc 1.0,    Test_acc 0.9787\n",
      "Epoch 65. Loss: 0.00011369416513547028,    Train_acc 1.0,    Test_acc 0.9787\n",
      "Epoch 66. Loss: 0.00011915798346557533,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 67. Loss: 0.00014217740412767477,    Train_acc 1.0,    Test_acc 0.9787\n",
      "Epoch 68. Loss: 0.00014487297741517705,    Train_acc 1.0,    Test_acc 0.9786\n",
      "Epoch 69. Loss: 0.00012860403669931548,    Train_acc 1.0,    Test_acc 0.9787\n",
      "Epoch 70. Loss: 0.00012097768995402903,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 71. Loss: 0.00012327573766625333,    Train_acc 1.0,    Test_acc 0.9788\n",
      "Epoch 72. Loss: 0.0001440377446513717,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 73. Loss: 0.00010702360509443175,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 74. Loss: 0.00011365669137180629,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 75. Loss: 0.00015799070092215246,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 76. Loss: 0.00011639194958232796,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 77. Loss: 0.00012667556843927788,    Train_acc 1.0,    Test_acc 0.979\n",
      "Epoch 78. Loss: 0.00010279078590805715,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 79. Loss: 0.00011271437740163438,    Train_acc 1.0,    Test_acc 0.9789\n",
      "Epoch 80. Loss: 0.00011236484513240447,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 81. Loss: 0.00012267692307920897,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 82. Loss: 9.599131669293312e-05,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 83. Loss: 0.00012222452690005873,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 84. Loss: 0.00011736903687542156,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 85. Loss: 8.876558962080502e-05,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 86. Loss: 0.00010083845099720796,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 87. Loss: 0.00010989524011544279,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 88. Loss: 0.00010570022726038627,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 89. Loss: 0.00010308082864338615,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 90. Loss: 0.00010200761103442579,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 91. Loss: 9.711604807339533e-05,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 92. Loss: 9.463694255801065e-05,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 93. Loss: 9.709881837045259e-05,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 94. Loss: 9.24027160846033e-05,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 95. Loss: 0.00010058378857496871,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 96. Loss: 9.545855410063772e-05,    Train_acc 1.0,    Test_acc 0.9791\n",
      "Epoch 97. Loss: 9.70211930794161e-05,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 98. Loss: 0.00010305003002925894,    Train_acc 1.0,    Test_acc 0.9792\n",
      "Epoch 99. Loss: 8.880938532710692e-05,    Train_acc 1.0,    Test_acc 0.9792\n"
     ]
    }
   ],
   "source": [
    "train(net,0.0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-JRBb-BztwM"
   },
   "outputs": [],
   "source": [
    "#REF:https://gluon.mxnet.io/chapter03_deep-neural-networks/mlp-dropout-scratch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dropout on MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
